{"cells":[{"cell_type":"markdown","metadata":{"id":"vZAvFVIAtFlq"},"source":["# Exploring Chat Templates with SmolLM2\n","\n","This notebook demonstrates how to use chat templates with the `SmolLM2` model. Chat templates help structure interactions between users and AI models, ensuring consistent and contextually appropriate responses."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K-lZu8JvtwUN"},"outputs":[],"source":["# Install the requirements in Google Colab\n","# !pip install transformers datasets trl huggingface_hub\n","\n","# Authenticate to Hugging Face\n","from huggingface_hub import login\n","\n","login()\n","\n","# for convenience you can create an environment variable containing your hub token as HF_TOKEN"]},{"cell_type":"code","source":["!pip install trl"],"metadata":{"id":"bsBI7vef76re"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"tnHzBR7vtFlr"},"outputs":[],"source":["# Import necessary libraries\n","\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","from trl import setup_chat_format\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"XTVOqbuetFlr"},"source":["## SmolLM2 Chat Template\n","\n","Let's explore how to use a chat template with the `SmolLM2` model. We'll define a simple conversation and apply the chat template."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nrxh0oX6tFls"},"outputs":[],"source":["# Dynamically set the device\n","device = (\n","    \"cuda\"\n","    if torch.cuda.is_available()\n","    else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",")\n","\n","model_name = \"HuggingFaceTB/SmolLM2-135M\"\n","model = AutoModelForCausalLM.from_pretrained(\n","    pretrained_model_name_or_path=model_name\n",").to(device)\n","tokenizer = AutoTokenizer.from_pretrained(pretrained_model_name_or_path=model_name)\n","model, tokenizer = setup_chat_format(model=model, tokenizer=tokenizer)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zkJwILrbtFls"},"outputs":[],"source":["# Define messages for SmolLM2\n","messages = [\n","    {\"role\": \"user\", \"content\": \"Hello, how are you?\"},\n","    {\n","        \"role\": \"assistant\",\n","        \"content\": \"I'm doing well, thank you! How can I assist you today?\",\n","    },\n","]"]},{"cell_type":"markdown","metadata":{"id":"Ve4dgtjstFls"},"source":["# Apply chat template without tokenization\n","\n","The tokenizer represents the conversation as a string with special tokens to describe the role of the user and the assistant.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pbAg-5x-tFls"},"outputs":[],"source":["input_text = tokenizer.apply_chat_template(messages, tokenize=False)\n","\n","print(\"Conversation with template:\", input_text)"]},{"cell_type":"markdown","metadata":{"id":"sfvdglOqtFls"},"source":["# Decode the conversation\n","\n","Note that the conversation is represented as above but with a further assistant message.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mXUVdPeytFls"},"outputs":[],"source":["input_text = tokenizer.apply_chat_template(\n","    messages, tokenize=True, add_generation_prompt=True\n",")\n","\n","print(\"Conversation decoded:\", tokenizer.decode(token_ids=input_text))"]},{"cell_type":"markdown","metadata":{"id":"UcZQpspEtFlt"},"source":["# Tokenize the conversation\n","\n","Of course, the tokenizer also tokenizes the conversation and special token as ids that relate to the model's vocabulary.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jc2PLxAMtFlt"},"outputs":[],"source":["input_text = tokenizer.apply_chat_template(messages, add_generation_prompt=True)\n","\n","print(\"Conversation tokenized:\", input_text)"]},{"cell_type":"markdown","metadata":{"id":"m3eNp9a0tFlt"},"source":["<div style='background-color: lightblue; padding: 10px; border-radius: 5px; margin-bottom: 20px; color:black'>\n","    <h2 style='margin: 0;color:blue'>Exercise: Process a dataset for SFT</h2>\n","    <p>Take a dataset from the Hugging Face hub and process it for SFT. </p>\n","    <p><b>Difficulty Levels</b></p>\n","    <p>üê¢ Convert the `HuggingFaceTB/smoltalk` dataset into chatml format.</p>\n","    <p>üêï Convert the `openai/gsm8k` dataset into chatml format.</p>\n","</div>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qbkXV2_ItFlt"},"outputs":[],"source":["from IPython.core.display import display, HTML\n","\n","display(\n","    HTML(\n","        \"\"\"<iframe\n","  src=\"https://huggingface.co/datasets/HuggingFaceTB/smoltalk/embed/viewer/all/train?row=0\"\n","  frameborder=\"0\"\n","  width=\"100%\"\n","  height=\"360px\"\n","></iframe>\n","\"\"\"\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4p3atw4_tFlu"},"outputs":[],"source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"HuggingFaceTB/smoltalk\", \"everyday-conversations\")\n","\n","\n","def process_dataset(sample):\n","    # TODO: üê¢ Convert the sample into a chat format\n","    # use the tokenizer's method to apply the chat template\n","    sample[\"messages\"] = tokenizer.apply_chat_template(sample[\"messages\"], tokenize=False)\n","    return sample\n","\n","\n","ds = ds.map(process_dataset)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"81fQeazltFlu"},"outputs":[],"source":["display(\n","    HTML(\n","        \"\"\"<iframe\n","  src=\"https://huggingface.co/datasets/openai/gsm8k/embed/viewer/main/train\"\n","  frameborder=\"0\"\n","  width=\"100%\"\n","  height=\"360px\"\n","></iframe>\n","\"\"\"\n","    )\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"bWUSv7NMtFlu"},"outputs":[],"source":["ds = load_dataset(\"openai/gsm8k\", \"main\")\n","\n","\n","def process_dataset(sample):\n","    # TODO: üêï Convert the sample into a chat format\n","\n","    # 1. create a message format with the role and content\n","\n","    # 2. apply the chat template to the samples using the tokenizer's method\n","    messages = [\n","        {\"role\": \"question\", \"content\": sample[\"question\"]},\n","        {\"role\": \"answer\", \"content\": sample[\"answer\"]},\n","    ]\n","    sample[\"messages\"] = tokenizer.apply_chat_template(messages, tokenize=False)\n","\n","    return sample\n","\n","\n","ds = ds.map(process_dataset)"]},{"cell_type":"markdown","metadata":{"id":"qlXCuRKotFlu"},"source":["## Conclusion\n","\n","This notebook demonstrated how to apply chat templates to different models, `SmolLM2`. By structuring interactions with chat templates, we can ensure that AI models provide consistent and contextually relevant responses.\n","\n","In the exercise you tried out converting a dataset into chatml format. Luckily, TRL will do this for you, but it's useful to understand what's going on under the hood."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"py310","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.15"}},"nbformat":4,"nbformat_minor":0}